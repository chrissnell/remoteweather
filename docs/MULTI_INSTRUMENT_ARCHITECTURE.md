# Multi-Instrument Architecture Design

## Executive Summary

This document outlines the architecture for transforming RemoteWeather from a weather-specific system into a flexible platform supporting multiple instrument classes including geophysical sensors, industrial monitors, and various weather station types. The design prioritizes extensibility without code changes, maintains backward compatibility, and preserves the performance characteristics of the current system.

## Problem Statement

The current RemoteWeather architecture is tightly coupled to weather data:
- `WeatherReading` struct with 100+ weather-specific fields
- Fixed TimescaleDB schema for weather data only
- Weather-specific gRPC protocol definitions
- Controllers, storage engines, and station drivers assume weather data

Adding new instrument types requires extensive code modifications, database schema changes that risk data loss, and creates maintenance burden for each new instrument class.

## Design Goals

1. **Configuration-Driven Development**: Add new instrument classes via YAML configuration, with automatic code generation and compilation
2. **Type Safety**: Maintain Go's compile-time type checking through generated strongly-typed structs
3. **Performance**: No degradation from current system performance
4. **Backward Compatibility**: Existing weather functionality continues unchanged
5. **Scalability**: Support hundreds of instrument classes without architectural strain

## Core Architecture

### Code Generation Requirement

Due to Go's strong typing, adding new instrument classes requires code generation and recompilation. The "zero-code" goal means developers don't write code manually, but the system automatically generates and compiles it:

```bash
# Workflow for adding a new instrument class
1. Create YAML definition: config/instruments/new_sensor.yaml
2. Run code generator: make generate-instruments
3. Rebuild binary: make build
4. Deploy new binary: make deploy
```

The code generation process creates:
- Go struct definitions for type safety
- Protocol buffer definitions and generated code  
- Database migration scripts
- API handler code
- Validation functions

### Code Generation Process

The code generator is a separate utility that transforms YAML instrument definitions into Go source code:

```bash
# Workflow for adding a new instrument class
1. Create YAML definition: config/instruments/new_sensor.yaml
2. Run code generator: make generate-instruments
3. Rebuild binary: make build
4. Deploy new binary: make deploy
```

The code generator (`cmd/instrument-gen/main.go`) performs these steps:

1. **Parse YAML**: Read all instrument definitions from `config/instruments/`
2. **Generate Go Structs**: Create strongly-typed structs embedding BaseReading
3. **Generate Protobuf**: Create `.proto` files and run `protoc` to generate gRPC code
4. **Generate Migrations**: Create SQL scripts for TimescaleDB tables
5. **Update Registry**: Generate registry initialization code that registers all classes
6. **Generate Validators**: Create validation functions based on YAML rules
7. **Update API Routes**: Generate routing code for REST and gRPC endpoints

```go
// Example of generated code from YAML definition
// generated/instruments/geophysical_seismic.go

package instruments

import (
    "time"
    "github.com/chrissnell/remoteweather/pkg/instruments"
)

// Code generated by instrument-gen. DO NOT EDIT.

type GeophysicalSeismicReading struct {
    instruments.BaseReading // Embeds Timestamp, StationName, UnitSystem, DataSource
    GroundVelocityX   float64   `json:"ground_velocity_x" db:"velocity_x"`
    GroundVelocityY   float64   `json:"ground_velocity_y" db:"velocity_y"`
    GroundVelocityZ   float64   `json:"ground_velocity_z" db:"velocity_z"`
    FrequencySpectrum []byte    `json:"frequency_spectrum,omitempty" db:"spectrum"`
}

func (g GeophysicalSeismicReading) GetInstrumentClass() string {
    return "geophysical_seismic"
}

func (g GeophysicalSeismicReading) GetTableName() string {
    return "readings_geophysical_seismic"
}

func (g GeophysicalSeismicReading) Validate() error {
    // Generated validation code
    return nil
}
```

This approach maintains Go's type safety while providing a configuration-driven workflow. The generated code is checked into version control, allowing code review and ensuring reproducible builds.

### Base Instrument Fields

All instrument classes share a common set of base fields that provide essential metadata:

```go
// pkg/instruments/base.go
type BaseReading struct {
    Timestamp    time.Time `json:"timestamp" db:"timestamp"`
    StationName  string    `json:"station_name" db:"station_name"`
    UnitSystem   string    `json:"unit_system" db:"unit_system"` // "imperial" or "metric"
    DataSource   string    `json:"data_source" db:"data_source"` // Input plugin or data sender network
}
```

These fields are automatically included in every generated instrument struct:
- **Timestamp**: When the reading was taken
- **StationName**: Unique identifier for the instrument/station
- **UnitSystem**: Whether measurements are in imperial or metric units
- **DataSource**: Identifies the input plugin (e.g., "davis_vp2", "modbus_tcp") or network source (e.g., "field_network_1", "customer_batch_upload")

### 1. Instrument Registry System

The instrument registry is the central component that manages all instrument class definitions:

```go
// pkg/instruments/registry/registry.go
package registry

type Registry struct {
    classes map[string]*InstrumentClass
    mu      sync.RWMutex
}

type InstrumentClass struct {
    Name            string                    `yaml:"name"`
    DisplayName     string                    `yaml:"display_name"`
    Description     string                    `yaml:"description"`
    TableName       string                    `yaml:"table_name"`
    ProtoPackage    string                    `yaml:"proto_package"`
    Fields          []FieldDefinition         `yaml:"fields"`
    Validators      map[string]ValidatorSpec  `yaml:"validators"`
}

type FieldDefinition struct {
    Name         string      `yaml:"name"`
    DBColumn     string      `yaml:"db_column"`
    ProtoNumber  int32       `yaml:"proto_number"`
    DataType     FieldType   `yaml:"data_type"`
    Unit         string      `yaml:"unit"`
    Required     bool        `yaml:"required"`
    DefaultValue interface{} `yaml:"default"`
    Validator    string      `yaml:"validator"`  // References a validator in Validators map
}

type ValidatorSpec struct {
    Type       string                 `yaml:"type"`       // "range", "regex", "enum", etc.
    Parameters map[string]interface{} `yaml:"parameters"` // Type-specific parameters
}
```

### 2. Validation System

The validation system ensures data quality by defining rules for each field. Validators are defined once in the data class YAML and compiled at startup for efficient runtime validation.

#### Validator Types

```go
// pkg/validation/types.go
type ValidationFunc func(value interface{}) bool

type ValidatorCompiler interface {
    Compile(spec ValidatorSpec) (ValidationFunc, error)
}

// Built-in validator types
type RangeValidator struct{}      // Numeric range validation
type RegexValidator struct{}      // String pattern validation  
type EnumValidator struct{}       // Allowed value list
type CustomValidator struct{}     // User-defined validation logic
```

#### Validation Compilation

Validators are "compiled" at startup - converting configuration into executable functions:

```go
// pkg/validation/compiler.go
func CompileValidator(spec ValidatorSpec) (ValidationFunc, error) {
    switch spec.Type {
    case "range":
        min := spec.Parameters["min"].(float64)
        max := spec.Parameters["max"].(float64)
        
        // Return a closure that captures min/max
        return func(value interface{}) bool {
            v, ok := value.(float64)
            if !ok {
                return false
            }
            return v >= min && v <= max
        }, nil
        
    case "regex":
        pattern := spec.Parameters["pattern"].(string)
        
        // Pre-compile the regex once
        re, err := regexp.Compile(pattern)
        if err != nil {
            return nil, err
        }
        
        // Return function using pre-compiled regex
        return func(value interface{}) bool {
            s, ok := value.(string)
            return ok && re.MatchString(s)
        }, nil
        
    case "enum":
        allowed := spec.Parameters["values"].([]interface{})
        allowedSet := make(map[interface{}]bool)
        for _, v := range allowed {
            allowedSet[v] = true
        }
        
        return func(value interface{}) bool {
            return allowedSet[value]
        }, nil
    }
}
```

#### Validation in Processors

Processors load and cache validators at startup for efficient runtime validation:

```go
// internal/processors/quality/processor.go
type QualityControlProcessor struct {
    registry   *registry.Registry
    validators map[string]map[string]ValidationFunc // dataClass -> fieldName -> validator
}

func NewQualityControlProcessor(registry *registry.Registry) (*QualityControlProcessor, error) {
    p := &QualityControlProcessor{
        registry:   registry,
        validators: make(map[string]map[string]ValidationFunc),
    }
    
    // Load all validators at startup
    for _, className := range p.SupportedClasses() {
        class, _ := registry.GetClass(className)
        p.validators[className] = make(map[string]ValidationFunc)
        
        for _, field := range class.Fields {
            if field.Validator != "" {
                validatorSpec := class.Validators[field.Validator]
                fn, _ := CompileValidator(validatorSpec)
                p.validators[className][field.Name] = fn
            }
        }
    }
    return p, nil
}

func (q *QualityControlProcessor) Process(reading Reading) (Reading, error) {
    classValidators := q.validators[reading.InstrumentClass]
    
    for fieldName, value := range reading.Data {
        if validator, exists := classValidators[fieldName]; exists {
            if !validator(value) {
                reading.Data[fieldName + "_quality"] = "FAILED"
            }
        }
    }
    return reading, nil
}
```

### 3. Input/Output/Processor System

Transform the current architecture into a flexible pipeline system:

```go
// internal/pipeline/interfaces.go
package pipeline

// Input sources produce readings
type Input interface {
    // Lifecycle
    Initialize(ctx context.Context, config InputConfig) error
    Start(ctx context.Context, wg *sync.WaitGroup) chan<- Reading
    Stop() error
    
    // Capabilities
    AcceptsClasses() []string  // Which classes this input can receive
    GetMetrics() InputMetrics
}

// Processors transform readings
type Processor interface {
    Process(reading Reading) (Reading, error)
    SupportedClasses() []string
}

// Outputs (sinks) consume readings
type Output interface {
    // Lifecycle
    Initialize(ctx context.Context, config OutputConfig) error
    Start(ctx context.Context, wg *sync.WaitGroup) chan<- Reading
    Stop() error
    
    // Capabilities
    SupportedClasses() []string  // Which classes this output can handle
    GetMetrics() OutputMetrics
}

// Example: gRPC receiver as generic input
// internal/inputs/grpcreceiver/receiver.go
type GRPCReceiver struct {
    acceptsAll bool  // Can receive any instrument class
}

func (g *GRPCReceiver) AcceptsClasses() []string {
    if g.acceptsAll {
        return []string{"*"}  // Wildcard for any class
    }
    return g.configuredClasses
}

// Example: PWS Weather as output/sink
// internal/outputs/pwsweather/sink.go
type PWSWeatherSink struct{}

func (p *PWSWeatherSink) SupportedClasses() []string {
    return []string{
        "weather_davis",
        "weather_generic", 
        "weather_vantage",
        "weather_cumulus",
    }
}
```

### 3. Data Flow Architecture

#### Reading Pipeline

```go
// internal/pipeline/reading.go
type Reading struct {
    Timestamp       time.Time
    StationName     string
    InstrumentClass string
    Data            map[string]interface{}
}

type Pipeline struct {
    registry    *registry.Registry
    validators  *validation.Engine
    router      *Router
}

func (p *Pipeline) ProcessReading(r Reading) error {
    // 1. Identify instrument class
    class, err := p.registry.GetClass(r.InstrumentClass)
    if err != nil {
        return fmt.Errorf("unknown instrument class: %s", r.InstrumentClass)
    }
    
    // 2. Validate against class schema
    if err := p.validators.Validate(r, class); err != nil {
        return fmt.Errorf("validation failed: %w", err)
    }
    
    // 3. Route to appropriate storage engines
    return p.router.Route(r, class)
}
```

### 4. Storage Engine Adaptations

#### Generic Storage Interface

```go
// internal/storage/interface.go
type StorageEngine interface {
    // Lifecycle
    Initialize(ctx context.Context, config StorageConfig) error
    Start(ctx context.Context, wg *sync.WaitGroup) chan<- Reading
    Stop() error
    
    // Capabilities
    SupportsClass(className string) bool
    RegisterClass(class *registry.InstrumentClass) error
    
    // Operations
    StoreReading(reading Reading) error
    QueryReadings(query Query) ([]Reading, error)
}
```

#### TimescaleDB Multi-Table Support

```go
// internal/storage/timescaledb/multitable.go
type MultiTableStorage struct {
    db          *gorm.DB
    tables      map[string]*TableHandler
    registry    *registry.Registry
}

func (m *MultiTableStorage) RegisterClass(class *registry.InstrumentClass) error {
    // Create table if not exists
    if err := m.createTable(class); err != nil {
        return err
    }
    
    // Create hypertable
    if err := m.createHypertable(class.TableName); err != nil {
        return err
    }
    
    // Register table handler
    m.tables[class.Name] = &TableHandler{
        tableName: class.TableName,
        fields:    class.Fields,
    }
    
    return nil
}

func (m *MultiTableStorage) StoreReading(r Reading) error {
    handler, ok := m.tables[r.InstrumentClass]
    if !ok {
        return fmt.Errorf("no table handler for class: %s", r.InstrumentClass)
    }
    
    return handler.Insert(m.db, r)
}
```

### 5. Controller Adaptations

Controllers can declare which instrument classes they support:

```go
// internal/controllers/base/controller.go
type Controller interface {
    Name() string
    SupportedClasses() []string
    HandleReading(reading Reading) error
}

// internal/controllers/restserver/server.go
type RESTServer struct {
    registry *registry.Registry
    router   *mux.Router
}

func (s *RESTServer) RegisterClass(class *registry.InstrumentClass) {
    // Create endpoints for this class
    s.router.HandleFunc(
        fmt.Sprintf("/api/v1/%s/latest", class.Name),
        s.handleLatestReading(class),
    ).Methods("GET")
    
    s.router.HandleFunc(
        fmt.Sprintf("/api/v1/%s/query", class.Name),
        s.handleQuery(class),
    ).Methods("POST")
}
```

### 6. gRPC Service Evolution

#### Dynamic Service Generation

```go
// internal/grpc/generator/generator.go
type ServiceGenerator struct {
    registry *registry.Registry
}

func (g *ServiceGenerator) GenerateServices() error {
    for _, class := range g.registry.GetClasses() {
        // Generate proto file
        protoContent := g.generateProto(class)
        protoPath := fmt.Sprintf("protocols/%s/%s.proto", class.Name, class.Name)
        
        if err := os.WriteFile(protoPath, protoContent, 0644); err != nil {
            return err
        }
        
        // Generate Go code
        if err := g.runProtoc(protoPath); err != nil {
            return err
        }
    }
    return nil
}
```

#### Generic gRPC Handler

```go
// internal/grpc/handler/generic.go
type GenericHandler struct {
    registry *registry.Registry
    storage  storage.StorageEngine
}

func (h *GenericHandler) SendReadings(stream GenericReadingStream) error {
    for {
        reading, err := stream.Recv()
        if err == io.EOF {
            return stream.SendAndClose(&Empty{})
        }
        if err != nil {
            return err
        }
        
        // Convert generic reading to internal format
        internalReading := h.convertReading(reading)
        
        // Process through pipeline
        if err := h.pipeline.ProcessReading(internalReading); err != nil {
            log.Errorf("Failed to process reading: %v", err)
            continue
        }
    }
}
```

## Configuration Examples

### Weather Station Class

```yaml
# config/instruments/weather_davis.yaml
name: weather_davis
display_name: "Davis Weather Station"
description: "Davis Instruments weather station data"
table_name: readings_weather_davis
proto_package: weather.davis.v1

fields:
  - name: temperature_outside
    db_column: temp_out
    proto_number: 1
    data_type: float32
    unit: "celsius"
    required: true
    validator: temperature_range
    
  - name: humidity_outside
    db_column: humidity_out
    proto_number: 2
    data_type: float32
    unit: "percent"
    required: true
    validator: percentage

validators:
  temperature_range:
    type: range
    parameters:
      min: -50
      max: 60
  percentage:
    type: range
    parameters:
      min: 0
      max: 100
```

### Geophysical Sensor Class

```yaml
# config/instruments/geophysical_seismic.yaml
name: geophysical_seismic
display_name: "Seismic Sensor"
description: "Ground motion and seismic activity data"
table_name: readings_geophysical_seismic
proto_package: geophysical.seismic.v1

fields:
  - name: ground_velocity_x
    db_column: velocity_x
    proto_number: 1
    data_type: float64
    unit: "m/s"
    required: true
    
  - name: ground_velocity_y
    db_column: velocity_y
    proto_number: 2
    data_type: float64
    unit: "m/s"
    required: true
    
  - name: ground_velocity_z
    db_column: velocity_z
    proto_number: 3
    data_type: float64
    unit: "m/s"
    required: true
    
  - name: frequency_spectrum
    db_column: spectrum
    proto_number: 4
    data_type: json
    required: false

validators:
  velocity_range:
    type: range
    parameters:
      min: -100
      max: 100
  spectrum_format:
    type: regex
    parameters:
      pattern: "^\{.*\}$"  # Must be valid JSON
```

## Clean-Slate Implementation Strategy

### Repository Structure

We will start fresh with a new repository structure that supports the multi-instrument architecture from the ground up:

```
remoteweather-v2/
├── cmd/
│   ├── remoteweather/          # Main application
│   ├── instrument-gen/         # Code generator tool
│   ├── migrate/               # Database migration tool
│   ├── config-convert/        # YAML to SQLite converter (from v1)
│   └── instrument-convert/    # Instrument YAML to SQLite converter
├── config/
│   ├── instruments/           # Instrument class definitions (YAML for initial import only)
│   │   ├── weather_generic.yaml
│   │   └── examples/
│   └── remoteweather.db      # SQLite configuration database
├── pkg/
│   ├── config/              # Configuration providers
│   │   ├── provider.go      # Provider interface
│   │   ├── provider_sqlite.go # SQLite provider
│   │   └── provider_postgres.go # PostgreSQL provider (optional)
│   ├── migrate/            # Migration system (from v1)
│   ├── instruments/       # Public instrument interfaces
│   ├── pipeline/         # Pipeline interfaces
│   └── registry/        # Registry interfaces
├── internal/
│   ├── generator/          # Code generation logic
│   ├── pipeline/          # Pipeline implementation
│   ├── registry/         # Registry implementation
│   ├── inputs/          # Input implementations
│   ├── processors/     # Processor implementations
│   └── outputs/       # Output implementations
├── migrations/
│   ├── config/          # SQLite schema migrations
│   │   ├── 001_initial_schema.*.sql (from v1)
│   │   ├── 002_instrument_tables.up.sql
│   │   └── 002_instrument_tables.down.sql
│   └── instruments/    # Generated TimescaleDB migrations
├── generated/           # Generated code (git-ignored initially)
│   ├── instruments/    # Generated structs
│   ├── proto/         # Generated protobuf
│   └── migrations/   # Generated SQL
└── tools/           # Build tools and scripts
```

### Implementation Phases

#### Phase 1: Foundation (Week 1)
**Goal**: Create the basic scaffolding and pipeline architecture

1. **Set up new repository**
   ```bash
   # Option 1: New repository
   git init remoteweather-v2
   
   # Option 2: New branch with cleaned history
   git checkout --orphan v2-multi-instrument
   git rm -rf .
   ```

2. **Create core interfaces**
   ```go
   // pkg/pipeline/interfaces.go
   type Reading interface {
       GetTimestamp() time.Time
       GetStationName() string
       GetInstrumentClass() string
       Validate() error
   }
   
   type Input interface {
       Start(context.Context) (<-chan Reading, error)
   }
   
   type Output interface {
       Start(context.Context) (chan<- Reading, error)
   }
   ```

3. **Implement basic pipeline**
   ```go
   // internal/pipeline/pipeline.go
   type Pipeline struct {
       inputs     []Input
       processors []Processor
       outputs    []Output
   }
   ```

#### Phase 2: Code Generation (Week 2)
**Goal**: Implement the instrument code generator

1. **Create generator tool**
   ```go
   // cmd/instrument-gen/main.go
   // Reads YAML definitions and generates:
   // - Go structs (with embedded BaseReading)
   // - Proto files
   // - SQL migrations
   ```

2. **Define weather_generic.yaml**
   ```yaml
   name: weather_generic
   display_name: "Generic Weather Station"
   description: "Generic weather station supporting common measurements"
   table_name: readings_weather_generic
   proto_package: weather.generic.v1
   
   # Note: Base fields (Timestamp, StationName, UnitSystem, DataSource) 
   # are automatically included in all generated structs
   fields:
     - name: temperature
       type: float32
       db_column: temperature
       unit: "degrees" # Unit will be interpreted based on UnitSystem
     - name: humidity
       type: float32
       db_column: humidity
       unit: "percent"
   ```

3. **Generate code from YAML**
   ```bash
   # Generator reads directly from YAML files
   go run cmd/instrument-gen/main.go -dir config/instruments/
   # Creates: 
   # - generated/instruments/weather_generic.go (with BaseReading embedded)
   # - generated/proto/weather_generic.proto
   # - generated/migrations/weather_generic.sql
   ```

#### Phase 3: Dummy Implementation (Week 3)
**Goal**: Prove the pipeline works with simple components

1. **Implement dummy input**
   ```go
   // internal/inputs/dummy/dummy.go
   type DummyInput struct {
       interval time.Duration
       class    string
   }
   
   func (d *DummyInput) Start(ctx context.Context) (<-chan pipeline.Reading, error) {
       // Generate fake readings every interval
   }
   ```

2. **Implement dummy output**
   ```go
   // internal/outputs/console/console.go
   type ConsoleOutput struct{}
   
   func (c *ConsoleOutput) Start(ctx context.Context) (chan<- pipeline.Reading, error) {
       // Print readings to stdout
   }
   ```

3. **Wire together in main**
   ```go
   // cmd/remoteweather/main.go
   func main() {
       // Create pipeline with dummy input → console output
       // Verify readings flow through
   }
   ```

#### Phase 4: Real Components (Week 4)
**Goal**: Implement first real input and output

1. **Implement TimescaleDB output**
   - Port existing TimescaleDB code to new architecture
   - Support dynamic table creation from generated schemas
   - Test with weather_generic class

2. **Implement gRPC receiver input**
   - Create generic gRPC service
   - Use generated protobuf definitions
   - Support any instrument class

3. **Expand weather_generic.yaml**
   - Add more realistic weather fields
   - Test code generation with complex types
   - Verify database table creation

#### Phase 5: Migration Bridge (Week 5)
**Goal**: Create compatibility layer for existing code

1. **Import select components from v1**
   - Device management
   - Configuration system
   - Logging infrastructure

2. **Create v1 compatibility mode**
   - Map old WeatherReading to weather_generic class
   - Support existing gRPC endpoints
   - Database view for backward compatibility

### Development Workflow

```bash
# Day 1: Initialize new structure
mkdir remoteweather-v2
cd remoteweather-v2
go mod init github.com/chrissnell/remoteweather/v2

# Day 2-3: Core interfaces and pipeline
mkdir -p pkg/{pipeline,instruments,registry}
mkdir -p internal/{pipeline,generator}
# Implement basic pipeline that can route readings

# Day 4-5: Code generator
mkdir -p cmd/instrument-gen
# Build YAML parser and code generator

# Week 2: Dummy components
mkdir -p internal/inputs/dummy
mkdir -p internal/outputs/console
# Verify pipeline works end-to-end

# Week 3: Real components
mkdir -p internal/outputs/timescaledb
mkdir -p internal/inputs/grpcreceiver
# Port and adapt existing code

# Week 4+: Incremental migration
# Bring over components one by one
# Each component adapted to new architecture
```

### Key Decisions

1. **Fresh Start vs Branch**
   - Recommend: New branch with `--orphan` flag
   - Keeps history accessible but starts clean
   - Easy to reference old code during migration

2. **Generated Code Management**
   - Initially git-ignore generated code
   - Once stable, commit generated code
   - Allows CI/CD without generation step

3. **Testing Strategy**
   - Unit tests for each component
   - Integration tests for pipeline
   - End-to-end tests with dummy components first

4. **Migration Approach**
   - New code first, migrate later
   - Maintain v1 compatibility mode
   - Gradual transition over several releases

5. **Configuration Backend**
   - YAML files define instrument classes and are used by code generator
   - Generated Go code is compiled into the binary
   - No runtime configuration database needed for instrument definitions
   - All instrument metadata is compile-time generated

## Implementation Roadmap (Original Plan)

### Phase 1: Core Infrastructure (Weeks 1-2)
1. Implement instrument registry
2. Create configuration loader
3. Build validation framework
4. Design generic reading format

### Phase 2: Input System Refactor (Weeks 3-4)
1. Create input interface abstraction
2. Migrate existing weather stations to input drivers
3. Implement input registry
4. Build input router

### Phase 3: Storage Adaptation (Weeks 5-6)
1. Implement multi-table TimescaleDB support
2. Create dynamic table management
3. Update materialized view generation
4. Build query router

### Phase 4: API Evolution (Weeks 7-8)
1. Create dynamic REST endpoints
2. Implement generic gRPC service
3. Build proto generation system
4. Update client libraries

### Phase 5: Migration (Weeks 9-10)
1. Create migration tools
2. Build backward compatibility layer
3. Migrate existing weather data
4. Update documentation

## Migration Strategy

### Backward Compatibility

The system maintains full backward compatibility through:

1. **Legacy Endpoints**: Keep existing `/api/v1/weather/*` endpoints
2. **Proto Aliases**: Map old WeatherReading to new weather_* classes
3. **Table Views**: Create views mapping old schema to new tables
4. **Configuration Bridge**: Auto-convert old config to new format

### Data Migration

```sql
-- Create new weather table
CREATE TABLE readings_weather_generic AS 
SELECT 
    time as timestamp,
    stationname as station_name,
    'weather_generic' as instrument_class,
    jsonb_build_object(
        'temperature_outside', outtemp,
        'humidity_outside', outhumidity,
        -- map all fields
    ) as data
FROM weather;

-- Create backward-compatible view
CREATE VIEW weather AS
SELECT 
    timestamp as time,
    station_name as stationname,
    (data->>'temperature_outside')::float4 as outtemp,
    (data->>'humidity_outside')::float4 as outhumidity,
    -- map all fields back
FROM readings_weather_generic;
```

## Benefits

1. **Extensibility**: Add new instruments without touching code
2. **Maintainability**: Centralized configuration management
3. **Performance**: Dedicated tables per instrument class
4. **Flexibility**: Mix and match inputs, storage, and APIs
5. **Type Safety**: Generated code maintains compile-time checks

## Example: Adding Industrial Sensor

```bash
# 1. Create instrument definition
cat > config/instruments/industrial_pressure.yaml << EOF
name: industrial_pressure
display_name: "Industrial Pressure Monitor"
table_name: readings_industrial_pressure
proto_package: industrial.pressure.v1

# Note: Base fields are automatically included
fields:
  - name: pressure
    data_type: float64
    db_column: pressure
    unit: "bar"  # Always in bar regardless of UnitSystem
    validator: pressure_range
  - name: temperature
    data_type: float32
    db_column: temperature  
    unit: "degrees"  # Will be C or F based on UnitSystem
    validator: industrial_temp_range

validators:
  pressure_range:
    type: range
    parameters:
      min: 0
      max: 10  # 10 bar max
  industrial_temp_range:
    type: range
    parameters:
      min: -40
      max: 125  # Industrial temperature range
EOF

# 2. Generate code from YAML
make generate-instruments
# This creates:
# - generated/instruments/industrial_pressure.go (struct with embedded BaseReading)
# - protocols/industrial/pressure.proto (protobuf definition)
# - migrations/instruments/industrial_pressure.sql (table schema)

# 3. Compile new binary with generated code
make build

# 4. Run database migrations
make migrate-db

# 5. Deploy and restart service
make deploy
systemctl restart remoteweather

# 6. Send data via gRPC (using generated types)
grpcurl -d '{
  "timestamp": "2024-01-15T10:30:00Z",
  "station_name": "factory_01",
  "unit_system": "metric",
  "data_source": "modbus_tcp",
  "pressure": 2.5,
  "temperature": 45.2
}' localhost:50051 industrial.pressure.v1.IndustrialPressure/SendReading
```

## Conclusion

This architecture transforms RemoteWeather into a flexible, extensible platform while maintaining its core strengths. The design allows for growth from dozens to hundreds of instrument types without architectural changes, providing a solid foundation for future expansion.