# Multi-Instrument Architecture Design

## Executive Summary

This document outlines the architecture for transforming RemoteWeather from a weather-specific system into a flexible platform supporting multiple instrument classes including geophysical sensors, industrial monitors, and various weather station types. The design prioritizes extensibility without code changes, maintains backward compatibility, and preserves the performance characteristics of the current system.

## Problem Statement

The current RemoteWeather architecture is tightly coupled to weather data:
- `WeatherReading` struct with 100+ weather-specific fields
- Fixed TimescaleDB schema for weather data only
- Weather-specific gRPC protocol definitions
- Controllers, storage engines, and station drivers assume weather data

Adding new instrument types requires extensive code modifications, database schema changes that risk data loss, and creates maintenance burden for each new instrument class.

## Design Goals

1. **Configuration-Driven Development**: Add new instrument classes via YAML configuration, with automatic code generation and compilation
2. **Type Safety**: Maintain Go's compile-time type checking through generated strongly-typed structs
3. **Performance**: No degradation from current system performance
4. **Backward Compatibility**: Existing weather functionality continues unchanged
5. **Scalability**: Support hundreds of instrument classes without architectural strain

## Core Architecture

### Code Generation Requirement

Due to Go's strong typing, adding new instrument classes requires code generation and recompilation. The "zero-code" goal means developers don't write code manually, but the system automatically generates and compiles it:

```bash
# Workflow for adding a new instrument class
1. Create YAML definition: config/instruments/new_sensor.yaml
2. Run code generator: make generate-instruments
3. Rebuild binary: make build
4. Deploy new binary: make deploy
```

The code generation process creates:
- Go struct definitions for type safety
- Protocol buffer definitions and generated code  
- Database migration scripts
- API handler code
- Validation functions

```go
// Example of generated code from YAML definition
// generated/instruments/geophysical_seismic.go

package instruments

import (
    "time"
    "github.com/chrissnell/remoteweather/internal/types"
)

// Code generated by instrument-gen. DO NOT EDIT.

type GeophysicalSeismicReading struct {
    Timestamp         time.Time `json:"timestamp" db:"timestamp"`
    StationName       string    `json:"station_name" db:"station_name"`
    GroundVelocityX   float64   `json:"ground_velocity_x" db:"velocity_x"`
    GroundVelocityY   float64   `json:"ground_velocity_y" db:"velocity_y"`
    GroundVelocityZ   float64   `json:"ground_velocity_z" db:"velocity_z"`
    FrequencySpectrum []byte    `json:"frequency_spectrum,omitempty" db:"spectrum"`
}

func (g GeophysicalSeismicReading) GetInstrumentClass() string {
    return "geophysical_seismic"
}

func (g GeophysicalSeismicReading) GetTableName() string {
    return "readings_geophysical_seismic"
}

func (g GeophysicalSeismicReading) Validate() error {
    // Generated validation code
    return nil
}
```

### Configuration Backend: SQLite

Building on the existing SQLite configuration backend (see `docs/SQLITE_CONFIG_BACKEND.md`), we'll extend it to support instrument definitions:

#### Schema Extensions

```sql
-- Add to existing SQLite schema
CREATE TABLE instrument_classes (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT UNIQUE NOT NULL,
    display_name TEXT,
    description TEXT,
    table_name TEXT NOT NULL,
    proto_package TEXT,
    version INTEGER DEFAULT 1,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE instrument_fields (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    class_id INTEGER NOT NULL,
    name TEXT NOT NULL,
    db_column TEXT NOT NULL,
    proto_number INTEGER NOT NULL,
    data_type TEXT NOT NULL,
    unit TEXT,
    required BOOLEAN DEFAULT FALSE,
    default_value TEXT,
    validator_id INTEGER,
    FOREIGN KEY (class_id) REFERENCES instrument_classes(id) ON DELETE CASCADE,
    UNIQUE(class_id, name),
    UNIQUE(class_id, proto_number)
);

CREATE TABLE instrument_validators (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT UNIQUE NOT NULL,
    type TEXT NOT NULL,
    parameters TEXT -- JSON configuration
);

CREATE TABLE class_inputs (
    class_id INTEGER NOT NULL,
    input_type TEXT NOT NULL,
    FOREIGN KEY (class_id) REFERENCES instrument_classes(id) ON DELETE CASCADE,
    PRIMARY KEY (class_id, input_type)
);

CREATE TABLE class_outputs (
    class_id INTEGER NOT NULL,
    output_type TEXT NOT NULL,
    FOREIGN KEY (class_id) REFERENCES instrument_classes(id) ON DELETE CASCADE,
    PRIMARY KEY (class_id, output_type)
);
```

#### Configuration Provider Extension

```go
// pkg/config/provider_sqlite.go extensions
type SQLiteProvider struct {
    db *sql.DB
    // ... existing fields ...
}

func (s *SQLiteProvider) GetInstrumentClasses() ([]InstrumentClass, error) {
    // Query instrument definitions from SQLite
}

func (s *SQLiteProvider) RegisterInstrumentClass(class InstrumentClass) error {
    // Insert new instrument class with transaction safety
}

func (s *SQLiteProvider) UpdateInstrumentClass(class InstrumentClass) error {
    // Update existing class, increment version
}
```

#### Migration from YAML Definitions

```bash
# Convert YAML instrument definitions to SQLite
./instrument-convert -yaml config/instruments/*.yaml -sqlite config.db

# The converter will:
# 1. Parse YAML instrument definitions
# 2. Validate field definitions
# 3. Insert into SQLite with proper relationships
# 4. Generate migration scripts for TimescaleDB tables
```

#### Benefits of SQLite for Instrument Configuration

1. **Atomic Updates**: Change instrument definitions transactionally
2. **Validation**: Database constraints ensure valid configurations
3. **Querying**: SQL queries to find instruments by capability
4. **Versioning**: Track changes to instrument definitions
5. **Relationships**: Properly model inputs/outputs/processors

#### PostgreSQL Provider Option

For production deployments requiring centralized configuration management:

```go
// pkg/config/provider_postgres.go
type PostgresProvider struct {
    db *sql.DB
    // Implements same ConfigProvider interface as SQLite
}

// Benefits over SQLite:
// - Multi-instance configuration sharing
// - Concurrent access from multiple RemoteWeather instances
// - Integration with existing PostgreSQL/TimescaleDB infrastructure
// - Advanced features: replication, point-in-time recovery
```

The same schema and interfaces work with PostgreSQL, allowing deployment flexibility:
- **Development/Single-instance**: SQLite for simplicity
- **Production/Multi-instance**: PostgreSQL for centralized management

### 1. Instrument Registry System

The instrument registry is the central component that manages all instrument class definitions:

```go
// pkg/instruments/registry/registry.go
package registry

type Registry struct {
    classes map[string]*InstrumentClass
    mu      sync.RWMutex
}

type InstrumentClass struct {
    Name            string                    `yaml:"name"`
    DisplayName     string                    `yaml:"display_name"`
    Description     string                    `yaml:"description"`
    TableName       string                    `yaml:"table_name"`
    ProtoPackage    string                    `yaml:"proto_package"`
    Fields          []FieldDefinition         `yaml:"fields"`
    Validators      map[string]ValidatorSpec  `yaml:"validators"`
    SupportedInputs []string                  `yaml:"supported_inputs"`
    StorageEngines  []string                  `yaml:"storage_engines"`
}

type FieldDefinition struct {
    Name         string      `yaml:"name"`
    DBColumn     string      `yaml:"db_column"`
    ProtoNumber  int32       `yaml:"proto_number"`
    DataType     FieldType   `yaml:"data_type"`
    Unit         string      `yaml:"unit"`
    Required     bool        `yaml:"required"`
    DefaultValue interface{} `yaml:"default"`
    Validator    string      `yaml:"validator"`
}
```

### 2. Input/Output/Processor System

Transform the current architecture into a flexible pipeline system:

```go
// internal/pipeline/interfaces.go
package pipeline

// Input sources produce readings
type Input interface {
    // Lifecycle
    Initialize(ctx context.Context, config InputConfig) error
    Start(ctx context.Context, wg *sync.WaitGroup) chan<- Reading
    Stop() error
    
    // Capabilities
    AcceptsClasses() []string  // Which classes this input can receive
    GetMetrics() InputMetrics
}

// Processors transform readings
type Processor interface {
    Process(reading Reading) (Reading, error)
    SupportedClasses() []string
}

// Outputs (sinks) consume readings
type Output interface {
    // Lifecycle
    Initialize(ctx context.Context, config OutputConfig) error
    Start(ctx context.Context, wg *sync.WaitGroup) chan<- Reading
    Stop() error
    
    // Capabilities
    SupportedClasses() []string  // Which classes this output can handle
    GetMetrics() OutputMetrics
}

// Example: gRPC receiver as generic input
// internal/inputs/grpcreceiver/receiver.go
type GRPCReceiver struct {
    acceptsAll bool  // Can receive any instrument class
}

func (g *GRPCReceiver) AcceptsClasses() []string {
    if g.acceptsAll {
        return []string{"*"}  // Wildcard for any class
    }
    return g.configuredClasses
}

// Example: PWS Weather as output/sink
// internal/outputs/pwsweather/sink.go
type PWSWeatherSink struct{}

func (p *PWSWeatherSink) SupportedClasses() []string {
    return []string{
        "weather_davis",
        "weather_generic", 
        "weather_vantage",
        "weather_cumulus",
    }
}
```

### 3. Data Flow Architecture

#### Reading Pipeline

```go
// internal/pipeline/reading.go
type Reading struct {
    Timestamp       time.Time
    StationName     string
    InstrumentClass string
    Data            map[string]interface{}
}

type Pipeline struct {
    registry    *registry.Registry
    validators  *validation.Engine
    router      *Router
}

func (p *Pipeline) ProcessReading(r Reading) error {
    // 1. Identify instrument class
    class, err := p.registry.GetClass(r.InstrumentClass)
    if err != nil {
        return fmt.Errorf("unknown instrument class: %s", r.InstrumentClass)
    }
    
    // 2. Validate against class schema
    if err := p.validators.Validate(r, class); err != nil {
        return fmt.Errorf("validation failed: %w", err)
    }
    
    // 3. Route to appropriate storage engines
    return p.router.Route(r, class)
}
```

### 4. Storage Engine Adaptations

#### Generic Storage Interface

```go
// internal/storage/interface.go
type StorageEngine interface {
    // Lifecycle
    Initialize(ctx context.Context, config StorageConfig) error
    Start(ctx context.Context, wg *sync.WaitGroup) chan<- Reading
    Stop() error
    
    // Capabilities
    SupportsClass(className string) bool
    RegisterClass(class *registry.InstrumentClass) error
    
    // Operations
    StoreReading(reading Reading) error
    QueryReadings(query Query) ([]Reading, error)
}
```

#### TimescaleDB Multi-Table Support

```go
// internal/storage/timescaledb/multitable.go
type MultiTableStorage struct {
    db          *gorm.DB
    tables      map[string]*TableHandler
    registry    *registry.Registry
}

func (m *MultiTableStorage) RegisterClass(class *registry.InstrumentClass) error {
    // Create table if not exists
    if err := m.createTable(class); err != nil {
        return err
    }
    
    // Create hypertable
    if err := m.createHypertable(class.TableName); err != nil {
        return err
    }
    
    // Register table handler
    m.tables[class.Name] = &TableHandler{
        tableName: class.TableName,
        fields:    class.Fields,
    }
    
    return nil
}

func (m *MultiTableStorage) StoreReading(r Reading) error {
    handler, ok := m.tables[r.InstrumentClass]
    if !ok {
        return fmt.Errorf("no table handler for class: %s", r.InstrumentClass)
    }
    
    return handler.Insert(m.db, r)
}
```

### 5. Controller Adaptations

Controllers can declare which instrument classes they support:

```go
// internal/controllers/base/controller.go
type Controller interface {
    Name() string
    SupportedClasses() []string
    HandleReading(reading Reading) error
}

// internal/controllers/restserver/server.go
type RESTServer struct {
    registry *registry.Registry
    router   *mux.Router
}

func (s *RESTServer) RegisterClass(class *registry.InstrumentClass) {
    // Create endpoints for this class
    s.router.HandleFunc(
        fmt.Sprintf("/api/v1/%s/latest", class.Name),
        s.handleLatestReading(class),
    ).Methods("GET")
    
    s.router.HandleFunc(
        fmt.Sprintf("/api/v1/%s/query", class.Name),
        s.handleQuery(class),
    ).Methods("POST")
}
```

### 6. gRPC Service Evolution

#### Dynamic Service Generation

```go
// internal/grpc/generator/generator.go
type ServiceGenerator struct {
    registry *registry.Registry
}

func (g *ServiceGenerator) GenerateServices() error {
    for _, class := range g.registry.GetClasses() {
        // Generate proto file
        protoContent := g.generateProto(class)
        protoPath := fmt.Sprintf("protocols/%s/%s.proto", class.Name, class.Name)
        
        if err := os.WriteFile(protoPath, protoContent, 0644); err != nil {
            return err
        }
        
        // Generate Go code
        if err := g.runProtoc(protoPath); err != nil {
            return err
        }
    }
    return nil
}
```

#### Generic gRPC Handler

```go
// internal/grpc/handler/generic.go
type GenericHandler struct {
    registry *registry.Registry
    storage  storage.StorageEngine
}

func (h *GenericHandler) SendReadings(stream GenericReadingStream) error {
    for {
        reading, err := stream.Recv()
        if err == io.EOF {
            return stream.SendAndClose(&Empty{})
        }
        if err != nil {
            return err
        }
        
        // Convert generic reading to internal format
        internalReading := h.convertReading(reading)
        
        // Process through pipeline
        if err := h.pipeline.ProcessReading(internalReading); err != nil {
            log.Errorf("Failed to process reading: %v", err)
            continue
        }
    }
}
```

## Configuration Examples

### Weather Station Class

```yaml
# config/instruments/weather_davis.yaml
name: weather_davis
display_name: "Davis Weather Station"
description: "Davis Instruments weather station data"
table_name: readings_weather_davis
proto_package: weather.davis.v1

fields:
  - name: temperature_outside
    db_column: temp_out
    proto_number: 1
    data_type: float32
    unit: "celsius"
    required: true
    validator: temperature_range
    
  - name: humidity_outside
    db_column: humidity_out
    proto_number: 2
    data_type: float32
    unit: "percent"
    required: true
    validator: percentage

validators:
  temperature_range:
    type: range
    min: -50
    max: 60
  percentage:
    type: range
    min: 0
    max: 100

supported_inputs:
  - davis_vp2
  - davis_envoy
  - grpc_generic
  
storage_engines:
  - timescaledb
  - influxdb
  - grpc_stream
```

### Geophysical Sensor Class

```yaml
# config/instruments/geophysical_seismic.yaml
name: geophysical_seismic
display_name: "Seismic Sensor"
description: "Ground motion and seismic activity data"
table_name: readings_geophysical_seismic
proto_package: geophysical.seismic.v1

fields:
  - name: ground_velocity_x
    db_column: velocity_x
    proto_number: 1
    data_type: float64
    unit: "m/s"
    required: true
    
  - name: ground_velocity_y
    db_column: velocity_y
    proto_number: 2
    data_type: float64
    unit: "m/s"
    required: true
    
  - name: ground_velocity_z
    db_column: velocity_z
    proto_number: 3
    data_type: float64
    unit: "m/s"
    required: true
    
  - name: frequency_spectrum
    db_column: spectrum
    proto_number: 4
    data_type: json
    required: false

supported_inputs:
  - campbell_scientific
  - modbus_tcp
  - grpc_generic
  
storage_engines:
  - timescaledb
  - elasticsearch
```

## Clean-Slate Implementation Strategy

### Repository Structure

We will start fresh with a new repository structure that supports the multi-instrument architecture from the ground up:

```
remoteweather-v2/
├── cmd/
│   ├── remoteweather/          # Main application
│   ├── instrument-gen/         # Code generator tool
│   ├── migrate/               # Database migration tool
│   ├── config-convert/        # YAML to SQLite converter (from v1)
│   └── instrument-convert/    # Instrument YAML to SQLite converter
├── config/
│   ├── instruments/           # Instrument class definitions (YAML for initial import only)
│   │   ├── weather_generic.yaml
│   │   └── examples/
│   └── remoteweather.db      # SQLite configuration database
├── pkg/
│   ├── config/              # Configuration providers
│   │   ├── provider.go      # Provider interface
│   │   ├── provider_sqlite.go # SQLite provider
│   │   └── provider_postgres.go # PostgreSQL provider (optional)
│   ├── migrate/            # Migration system (from v1)
│   ├── instruments/       # Public instrument interfaces
│   ├── pipeline/         # Pipeline interfaces
│   └── registry/        # Registry interfaces
├── internal/
│   ├── generator/          # Code generation logic
│   ├── pipeline/          # Pipeline implementation
│   ├── registry/         # Registry implementation
│   ├── inputs/          # Input implementations
│   ├── processors/     # Processor implementations
│   └── outputs/       # Output implementations
├── migrations/
│   ├── config/          # SQLite schema migrations
│   │   ├── 001_initial_schema.*.sql (from v1)
│   │   ├── 002_instrument_tables.up.sql
│   │   └── 002_instrument_tables.down.sql
│   └── instruments/    # Generated TimescaleDB migrations
├── generated/           # Generated code (git-ignored initially)
│   ├── instruments/    # Generated structs
│   ├── proto/         # Generated protobuf
│   └── migrations/   # Generated SQL
└── tools/           # Build tools and scripts
```

### Implementation Phases

#### Phase 1: Foundation (Week 1)
**Goal**: Create the basic scaffolding and pipeline architecture

1. **Set up new repository**
   ```bash
   # Option 1: New repository
   git init remoteweather-v2
   
   # Option 2: New branch with cleaned history
   git checkout --orphan v2-multi-instrument
   git rm -rf .
   ```

2. **Create core interfaces**
   ```go
   // pkg/pipeline/interfaces.go
   type Reading interface {
       GetTimestamp() time.Time
       GetStationName() string
       GetInstrumentClass() string
       Validate() error
   }
   
   type Input interface {
       Start(context.Context) (<-chan Reading, error)
   }
   
   type Output interface {
       Start(context.Context) (chan<- Reading, error)
   }
   ```

3. **Implement basic pipeline**
   ```go
   // internal/pipeline/pipeline.go
   type Pipeline struct {
       inputs     []Input
       processors []Processor
       outputs    []Output
   }
   ```

#### Phase 2: Code Generation (Week 2)
**Goal**: Implement the instrument code generator

1. **Create generator tool**
   ```go
   // cmd/instrument-gen/main.go
   // Reads YAML definitions and generates:
   // - Go structs
   // - Proto files
   // - SQL migrations
   ```

2. **Define weather_generic.yaml as bootstrap file**
   ```yaml
   # YAML files are only used for initial bootstrap/import
   # Not used at runtime - all configuration comes from database
   name: weather_generic
   fields:
     - name: temperature
       type: float32
       db_column: temperature
   ```

3. **Import to database configuration**
   ```bash
   # One-time import of instrument definition to database
   go run cmd/instrument-convert/main.go \
     -yaml config/instruments/weather_generic.yaml \
     -sqlite config/remoteweather.db
   
   # Or for PostgreSQL deployments:
   go run cmd/instrument-convert/main.go \
     -yaml config/instruments/weather_generic.yaml \
     -postgres "postgresql://user:pass@localhost/config"
   ```

4. **Generate code from SQLite**
   ```bash
   # Generator reads from SQLite, not YAML
   go run cmd/instrument-gen/main.go -sqlite config/remoteweather.db
   # Creates: 
   # - generated/instruments/weather_generic.go
   # - generated/proto/weather_generic.proto
   # - generated/migrations/weather_generic.sql
   ```

#### Phase 3: Dummy Implementation (Week 3)
**Goal**: Prove the pipeline works with simple components

1. **Implement dummy input**
   ```go
   // internal/inputs/dummy/dummy.go
   type DummyInput struct {
       interval time.Duration
       class    string
   }
   
   func (d *DummyInput) Start(ctx context.Context) (<-chan pipeline.Reading, error) {
       // Generate fake readings every interval
   }
   ```

2. **Implement dummy output**
   ```go
   // internal/outputs/console/console.go
   type ConsoleOutput struct{}
   
   func (c *ConsoleOutput) Start(ctx context.Context) (chan<- pipeline.Reading, error) {
       // Print readings to stdout
   }
   ```

3. **Wire together in main**
   ```go
   // cmd/remoteweather/main.go
   func main() {
       // Create pipeline with dummy input → console output
       // Verify readings flow through
   }
   ```

#### Phase 4: Real Components (Week 4)
**Goal**: Implement first real input and output

1. **Implement TimescaleDB output**
   - Port existing TimescaleDB code to new architecture
   - Support dynamic table creation from generated schemas
   - Test with weather_generic class

2. **Implement gRPC receiver input**
   - Create generic gRPC service
   - Use generated protobuf definitions
   - Support any instrument class

3. **Expand weather_generic.yaml**
   - Add more realistic weather fields
   - Test code generation with complex types
   - Verify database table creation

#### Phase 5: Migration Bridge (Week 5)
**Goal**: Create compatibility layer for existing code

1. **Import select components from v1**
   - Device management
   - Configuration system
   - Logging infrastructure

2. **Create v1 compatibility mode**
   - Map old WeatherReading to weather_generic class
   - Support existing gRPC endpoints
   - Database view for backward compatibility

### Development Workflow

```bash
# Day 1: Initialize new structure
mkdir remoteweather-v2
cd remoteweather-v2
go mod init github.com/chrissnell/remoteweather/v2

# Day 2-3: Core interfaces and pipeline
mkdir -p pkg/{pipeline,instruments,registry}
mkdir -p internal/{pipeline,generator}
# Implement basic pipeline that can route readings

# Day 4-5: Code generator
mkdir -p cmd/instrument-gen
# Build YAML parser and code generator

# Week 2: Dummy components
mkdir -p internal/inputs/dummy
mkdir -p internal/outputs/console
# Verify pipeline works end-to-end

# Week 3: Real components
mkdir -p internal/outputs/timescaledb
mkdir -p internal/inputs/grpcreceiver
# Port and adapt existing code

# Week 4+: Incremental migration
# Bring over components one by one
# Each component adapted to new architecture
```

### Key Decisions

1. **Fresh Start vs Branch**
   - Recommend: New branch with `--orphan` flag
   - Keeps history accessible but starts clean
   - Easy to reference old code during migration

2. **Generated Code Management**
   - Initially git-ignore generated code
   - Once stable, commit generated code
   - Allows CI/CD without generation step

3. **Testing Strategy**
   - Unit tests for each component
   - Integration tests for pipeline
   - End-to-end tests with dummy components first

4. **Migration Approach**
   - New code first, migrate later
   - Maintain v1 compatibility mode
   - Gradual transition over several releases

5. **Configuration Backend**
   - Database-only configuration (no YAML provider at runtime)
   - SQLite for development and single-instance deployments
   - PostgreSQL provider option for production/multi-instance
   - YAML files only used for initial bootstrap/import of definitions
   - Leverage existing SQLite implementation from current repo
   - Extend schema to support instrument definitions

## Implementation Roadmap (Original Plan)

### Phase 1: Core Infrastructure (Weeks 1-2)
1. Implement instrument registry
2. Create configuration loader
3. Build validation framework
4. Design generic reading format

### Phase 2: Input System Refactor (Weeks 3-4)
1. Create input interface abstraction
2. Migrate existing weather stations to input drivers
3. Implement input registry
4. Build input router

### Phase 3: Storage Adaptation (Weeks 5-6)
1. Implement multi-table TimescaleDB support
2. Create dynamic table management
3. Update materialized view generation
4. Build query router

### Phase 4: API Evolution (Weeks 7-8)
1. Create dynamic REST endpoints
2. Implement generic gRPC service
3. Build proto generation system
4. Update client libraries

### Phase 5: Migration (Weeks 9-10)
1. Create migration tools
2. Build backward compatibility layer
3. Migrate existing weather data
4. Update documentation

## Migration Strategy

### Backward Compatibility

The system maintains full backward compatibility through:

1. **Legacy Endpoints**: Keep existing `/api/v1/weather/*` endpoints
2. **Proto Aliases**: Map old WeatherReading to new weather_* classes
3. **Table Views**: Create views mapping old schema to new tables
4. **Configuration Bridge**: Auto-convert old config to new format

### Data Migration

```sql
-- Create new weather table
CREATE TABLE readings_weather_generic AS 
SELECT 
    time as timestamp,
    stationname as station_name,
    'weather_generic' as instrument_class,
    jsonb_build_object(
        'temperature_outside', outtemp,
        'humidity_outside', outhumidity,
        -- map all fields
    ) as data
FROM weather;

-- Create backward-compatible view
CREATE VIEW weather AS
SELECT 
    timestamp as time,
    station_name as stationname,
    (data->>'temperature_outside')::float4 as outtemp,
    (data->>'humidity_outside')::float4 as outhumidity,
    -- map all fields back
FROM readings_weather_generic;
```

## Benefits

1. **Extensibility**: Add new instruments without touching code
2. **Maintainability**: Centralized configuration management
3. **Performance**: Dedicated tables per instrument class
4. **Flexibility**: Mix and match inputs, storage, and APIs
5. **Type Safety**: Generated code maintains compile-time checks

## Example: Adding Industrial Sensor

```bash
# 1. Create instrument definition
cat > config/instruments/industrial_pressure.yaml << EOF
name: industrial_pressure
display_name: "Industrial Pressure Monitor"
table_name: readings_industrial_pressure
fields:
  - name: pressure
    data_type: float64
    unit: "bar"
  - name: temperature
    data_type: float32
    unit: "celsius"
EOF

# 2. Generate code from YAML
make generate-instruments
# This creates:
# - generated/instruments/industrial_pressure.go (struct definition)
# - protocols/industrial/pressure.proto (protobuf definition)
# - migrations/instruments/industrial_pressure.sql (table schema)

# 3. Compile new binary with generated code
make build

# 4. Run database migrations
make migrate-db

# 5. Deploy and restart service
make deploy
systemctl restart remoteweather

# 6. Send data via gRPC (using generated types)
grpcurl -d '{
  "instrument_class": "industrial_pressure",
  "station_name": "factory_01",
  "data": {
    "pressure": 2.5,
    "temperature": 45.2
  }
}' localhost:50051 generic.v1.Generic/SendReading
```

### What Happens Behind the Scenes

The code generator (`make generate-instruments`) performs these steps:

1. **Parse YAML**: Read all instrument definitions from `config/instruments/`
2. **Generate Go Structs**: Create strongly-typed structs for each instrument class
3. **Generate Protobuf**: Create `.proto` files and run `protoc` to generate gRPC code
4. **Generate Migrations**: Create SQL scripts for TimescaleDB tables
5. **Update Registry**: Generate registry initialization code that registers all classes
6. **Generate Validators**: Create validation functions based on YAML rules
7. **Update API Routes**: Generate routing code for REST and gRPC endpoints

This approach maintains Go's type safety while providing a configuration-driven workflow. The generated code is checked into version control, allowing code review and ensuring reproducible builds.

## Conclusion

This architecture transforms RemoteWeather into a flexible, extensible platform while maintaining its core strengths. The design allows for growth from dozens to hundreds of instrument types without architectural changes, providing a solid foundation for future expansion.